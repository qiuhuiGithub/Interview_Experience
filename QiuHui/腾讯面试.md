# 项目相关
略。
# 基础知识
1. seq2seq中，attention是怎么做的？
- 第一步，对于decoder端的输入，经过RNN得到一个向量，再和encoder端每个单词的编码计算一个分数，然后取softmax得到权重。分数计算的方式通常有两种，一种是加attention，一种是乘attention。第二步，得到attention权重之后，加权求和得到上下文向量。第三步，得到向量之后，再和decoder端得到的向量做交叉计算得到最终的attention向量，用于指导下一个字符的生成。以此循环。
2. 做attention时，如果encoder端和decoder端维度不一致，应该怎么做？
- 可以在把encoder的维度通过操作reshape到和decoder端一致，再进行操作。例如encoder端m\*n,decoder端m\*k,可以在encoder端输入的向量先乘以一个n*k的向量，再和decoder端做attention. 
3. word2vec中的负采样有了解吗？
- word2vec有两种常见的求解方法，分层softmax和负采样。例如针对cbow模型的负采样算法，正样本是将当前词的前后文和当前词组成的pair对，而负样本是当前词的前后文和随机采的词组成的pair对，以此来训练一个二分类模型。一种常见的负样本采样的做法是按权重采样，按照出现频率的权重把所有的单词分到一个固定长度的区间中，权重大的单词所占长度更长。然后使用一个较大的随机数，例如`10e8`在这个区间中随机采样一些数，看这些数字落在哪个区间，就采那几个词作为负样本。
4. bert和gpt有什么区别？
- 最主要的区别是bert使用的是双向transformer，而gpt使用的是单向transformer。因此bert具有较强的上下文理解能力，而gpt具有较强的文本生成能力。
5. 预训练语言模型推理阶段加速方法。
- 模型蒸馏。
- 使用小的预训练语言模型。
- 模型剪枝，减少参数。
- 降低精度，例如`Float32`变成`Float16`或者`Int8`.

# 编程题
字符串循环右移n位。

```
def move(s, n):
    if not s:
        return ''
    n = n % len(s)
    m = len(s) - n
    s = s[n+1:] + s[0:m]
    return s
```
 