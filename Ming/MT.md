## 一面

1. 介绍自己： 略
2. 介绍项目：略
3. 在项目介绍过程中，面试官抛的技术问题：
    1. 讲一下lambdaMART：
    - lambdamart是一种排序算法，它的前身是lambdarank，本质上是一种pair-wise的方法，比较的是两个候选item哪个更相关。
    - 1. lambda在这里就是我们常用的梯度（交叉熵损失）。lambdaMART是在lambdarank的基础上，对lambda直接定义，引入了排序指标ndcg。
    - 2. MART是指我们使用的是提升树模型。因为提升树模型本质上也是在优化负梯度，在lambdaMART里就是优化这个lambda值。
    2. 怎么判断特征重要性？
    - lightgbm有提供特征重要性计算的接口。LightGBM在计算特征重要性时，提供两种方式：（1）特征被用来分裂的次数；（2）特征用来分裂带来的总增益
    3. 增益怎么算的？
    - 左右子树的增益之和再减去当前节点的增益。增益的计算公式跟loss有关。源码里是用的-loss。
    4. sigmoid、交叉熵、softmax是什么关系
    - 1. sigmoid常用于二分类，模型的输出结果经过sigmoid得到为正样本的概率。softmax常用于多分类，经过softmax后得到模型判断为某个类别的概率。对于k等于2时，softmax就退化为sigmoid（自己后面想的一个问题：https://www.zhihu.com/question/295247085）
    - 2. 交叉熵常用作损失函数，衡量两个分布的相似度。
    5. 讲讲DCN
    - 1. DCN是一种并行结构的深度CTR常用模型，主要包括两个部分：学习显示高阶交叉的cross net和学习隐式高阶交叉的Deep部分。DCN相比于wide&Deep、DeepFM，都是在wide部分做了一些优化。
    - 2. DCN的cross net在做高阶交叉时，有两个比较特别的地方：（1）参数共享，降低复杂度（2）引入残差的思想，能避免一些网络退化的问题
    6. 讲讲梯度消失、梯度爆炸？怎么解决？
    - 1. 梯度消失、梯度爆炸是DNN比较经常遇到的问题，本质都是因为反向传播中的连乘效应。梯度消失是随着层数增加，如果激活函数求导的值小于1，连乘求出的梯度就会以指数衰减，最后梯度值接近0，导致参数无法更新。梯度爆炸是如果激活函数求导的值小于1，连乘求出的梯度就会以指数增加。
    - 2. 解决办法：（1）选择不饱和的激活函数，比如relu；（2）batch norm；（3）引入残差结构；（4）降低网络复杂度(主要是减少深度)

